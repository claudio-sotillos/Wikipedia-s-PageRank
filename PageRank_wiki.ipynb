{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "000ae955-cd3b-4a52-9e0c-978241c38d5c",
     "showTitle": true,
     "title": "                          "
    }
   },
   "source": [
    "#  WikiPedia's Page Rank Algorithm (Coded in PySpark)         \n",
    "#  Author: Claudio Sotillos Peceroso         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e4b91250-bb24-4024-bcb1-b99f7e8771f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from pyspark.sql.types import ArrayType, StringType,LongType\n",
    "from pyspark.sql.functions import collect_list, monotonically_increasing_id\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from operator import truediv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ae6143a-d173-4019-861b-e9955817c173",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e60d44ac-70ba-4da5-a8e9-3d52a7133d42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2493ee46-23a2-4a92-b83e-f2f028ca6dc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SOLUTION APPROACH: \n",
    "\n",
    "** Objective of the Work **\n",
    "\n",
    "The idea is to select a fraction of the wikipedia database and compute the page rank of every page contained in the wikipedia subset.<br>\n",
    "Since we will work with a large data set, we will use mainly Pyspark DFs, so that we can parallelize computations (making use of UDFs) and obtain the results in a shorter period of time. <br>\n",
    "We will only work with pages of Wikipedia. If, for instance, a wikipedia web page is pointing to a YouTube page, this won´t be taken into account for the page rank computation. \n",
    "\n",
    "### IMPORTANT REMARK <br>\n",
    "A given page can be in any of these situations: \n",
    " 1. It is a page which is only pointed by other pages/s.    \n",
    " 2. It is a page which point/s other page/s and is pointed by other page/s.\n",
    " 3. It is a page which only point/s to other page/s.\n",
    " 4. It is a page which doesn´t point to other page/s and which isn´t pointed by others. \n",
    "\n",
    "The Page Rank (I will use 'PR' many times to abbreviate) in the first two situations is computed in the same way, since for the PR computation what is needed is information with respect the pages which are pointing towards the Page which we want to compute its PR. Imagine that page A is pointed by pages B and C, then it´s PR computation would be:\n",
    "\n",
    "\\\\( PR_A= \\\\frac{PR_B}{L(B)}+ \\\\frac{PR_C}{L(C)}\\\\)   being L(B) the number of pages B is pointing (outgoing links) and \\\\(PR_B \\\\) the PR of B in the previous iteration (same for C). <br>\n",
    "\n",
    "**General Formula: **  \\\\(PR_i = \\sum \\\\frac{PR_j}{L(j)}\\\\)  being j the pages that point to i.\n",
    "\n",
    "Initially the PR of all the pages is initiallized to \\\\(\\\\frac{1}{N}\\\\), being N the total number of pages in the Wikipedia fraction which we are working with.\n",
    "\n",
    "***What happens with the pages which aren´t pointed by a single page? (cases 3 and 4 of the situations above) *** <br>\n",
    "\n",
    "Since these pages aren´t pointed by others, we can´t compute their PR with the above formula. That is why we must apply a slightly modified version of the above formula, including what is known as the **dumping factor**. Conceptually, the dumping factor is the probability that a user clicks in a link to another page. It is a constant value, typically 0.85. The new formula for the PR computation is (imagine the same scenario as before): \n",
    "\n",
    "\\\\( PR_A= \\\\frac{d}{N} + (1-d)(\\\\frac{PR_B}{L(B)}+ \\\\frac{PR_C}{L(C)}\\\\)) being d=0.85  and N= total number of pages in the Wikipedia fraction<br>\n",
    "\n",
    "**General Formula (This Formula will be used for the PR computation): **  \\\\(PR_i = \\\\frac{d}{N} + (1-d)(\\sum \\\\frac{PR_j}{L(j)}\\\\))\n",
    "\n",
    "Thus if a certain page isn´t pointed by others, the term \"\\\\((1-d)(\\sum \\\\frac{PR_j}{L(j)}\\\\))\" would be 0. <br>\n",
    "This means that these pages will always end having the same PR after the first iteration: \\\\(\\\\frac{d}{N}  \\\\). \n",
    "\n",
    "<br>\n",
    "###  How we will attain the solution  ?\n",
    "\n",
    "We start up with a Spark DataFrame which is a fraction of the original wikipedia database. \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>PartialWikipediaDF</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>title:string</td>\n",
    "  <td>id:integer</td>\n",
    "  <td>text:string</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Also we must create a broadcast Variable of a Pandas DataFrame, with the titles of all the pages in one column, and with their respective ID in another column. This variable will be called 'pages_index'.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>pages_index </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>title:string</td>\n",
    "  <td>id:integer</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "For attaining the solution we will follow the steps noted in the following MarkDown cells. \n",
    "\n",
    "*** Check Points (.cache) *** <br>\n",
    "Along the steps we will make some 'checkpoints' with the '.cache()' spark tool. The reason of doing this is because once the computations (of the instructions set before making the checkpoint) are done, these are stored in cache and don´t need to be recomputed again. Spark will just to take the needed information from cache memory. Of course, the check point will be done over inmutable datasets (datasets which values won´t change along the code execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29455663-e769-4c76-a3b0-370d20b553a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wikipediaDF=spark.read.parquet(\"dbfs:/databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet\") # Full wikipedia set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f1bd9c8-7f2f-4780-b338-8e55fbd947a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select a fraction of the Total wikipedia pages you want to pick\n",
    "PartialWikipediaDF=wikipediaDF.sample(fraction=0.01,seed=122370982).cache() # Fraction of full wikipedia set --> Fraction Limit: 0.000001\n",
    "PartialWikipediaDF = PartialWikipediaDF.dropDuplicates()  # Remove the duplicated instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9c9ce40-6272-43bc-b370-eb7f6ee6ecce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tolower_udf= udf(lambda x: x.lower()) #  UDF used to transform every title to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4551e13d-e9b3-4b02-aa26-e5e5c9b8c83e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generating the mappings of all the links in the Wkipedia DB with their respetive id\n",
    "intermediate_value=wikipediaDF.select(tolower_udf(wikipediaDF[\"title\"]).alias(\"title\"),wikipediaDF[\"id\"])\n",
    "pages_index=intermediate_value.toPandas()\n",
    "pages_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "662a5ec2-30a5-45c5-a8b1-2bd313f03e9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pages_index = pages_index.drop_duplicates() # Removing Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba28aa75-71c5-49db-a25a-365cf4657a68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We transform the pandas DF into a broadcast variable. A broadcast variable is a read-only variable, whose \n",
    "# information can be obtained by other machines working on parallel.\n",
    "# We will use this variable in the next UDF 'titles_to_id' which is used for mapping each of the links found in the text to its respective ID.\n",
    "broadcastVar = sc.broadcast(pages_index)  # Transform it into a broadcast variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45a591e5-ee20-4148-8bf0-e457a5f0c64b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd597042-9d23-4e03-9d50-c395a599ce21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** Step 1: Finding links inside the Text ** <br>\n",
    "Obtain the name of the links which are in the text of each of the pages. For doing this we will create a UDF (parse_links) which searches all link structures. Then, apply the UDF over the 'text' column and obtain as a result a column of arrays (links), each list containing the names of the outgoing links. In case that there isn´t a link in the text, we will return an empty list.  \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>Data</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>title:string</td>\n",
    "  <td>id:integer</td>\n",
    "  <td>links:array of strings</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18d3f39b-0d62-490a-919c-cb78cfa4393c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The 'parse_links' UDF is applied over the text field of each instance, and it saves in a list all the names of links (to other pages) found in the text. \n",
    "# The purpose of using UDFs is because we can make computations row by row in a much faster way since it can be parallelized.\n",
    "# (we can input one or more columns for applying the UDF over them)\n",
    "\n",
    "def parse_links(document_body):\n",
    "  data=re.findall(r'\\[\\[(.+?)\\]\\]',document_body)\n",
    "  \n",
    "  if (len(data)>0): # In case that more than one link is found in the text \n",
    "    links=[s.lower() for s in data]\n",
    "  else:   # In case there arent links in the text\n",
    "    links= []\n",
    "  return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e052fb5-9eb3-42d0-b10b-84f2bf8ea0cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parse_links_udf = udf(parse_links,ArrayType(StringType())) # Transform to UDF function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a86c28db-f7c8-4604-81a1-cd7687c91a92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select the Columns of interest (title, id, links found in the text) from the whole Partial Dataset. We apply 'parse_links_udf' over the text column \n",
    "# obtaining the corrersponding link names found on each page. \n",
    "data=PartialWikipediaDF.select(tolower_udf(PartialWikipediaDF[\"title\"]).alias(\"title\"),PartialWikipediaDF[\"id\"],parse_links_udf(PartialWikipediaDF[\"text\"]).alias(\"links\")).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c203417f-ad3e-42a1-8089-66692899e333",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** Step 2: Map the links to their ID ** <br>\n",
    "Now, we should create a UDF (titles_to_id) to transform the names of the link into their respective IDs. <br>\n",
    "Some of the names come splitted by a bar \"|\", this is because that page has different alias. Thus we must split the string into the different names it has and search which one is on our 'pages_index' variable (it can happen that none the alias are inside the variable, meaning that it isn´t a wikipedia page, or that more than one of the names are inside the variable. In the latter case, we don´t have to worry since we will delete duplicate values at the end of the function). <br>\n",
    "Other links will be unique (with that I mean that they are strings which aren´t divided by \"|\") and we will only have to search its name in the 'pages_index' variable. Also can happen that the list is empty and in that case an empty list will be returned too. \n",
    "\n",
    "Once we have created the UDF we apply it over the 'links' column obtaining a column of arrays of the corresponding link IDs. \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>Partialdirect_linksDF</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>id:integer</td>\n",
    "  <td>links_id:array of integers (the IDs)</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "87c04245-ecf9-41d9-91f8-7b99e0409ae7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This function will be a UDF which maps each list of outgoing links into its respective ids\n",
    "# i.e : Actual page id--> 345 ; links--> ['History','Mathematics'] ; titles_to_id_udf(title) --> [1234,9886]  (links_id)\n",
    "\n",
    "def titles_to_id(titles):\n",
    "\n",
    "  data_titles=broadcastVar.value  \n",
    "  # Split the pages which have alias names\n",
    "  result_clean = list(map(lambda sub: sub.split(\"|\"), titles))\n",
    "  result_clean = sum(result_clean, [])\n",
    "    \n",
    "  # To id\n",
    "  if (len(result_clean)>0):\n",
    "    ids=data_titles[data_titles.title.isin(result_clean)].id.to_list()\n",
    "    ids = list(set(ids))  # With this we ensure that aren´t duplicates in our ultimate list\n",
    "  else: # In case that there aren´t links in the text of the page \n",
    "    ids= [] \n",
    "  return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb041172-6c91-49f9-b3b9-84324393aeff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titles_to_id_udf=udf(titles_to_id,ArrayType(IntegerType())) # transform to UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ac5214c7-db4f-42a8-b3fa-f90b7a66a48b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply the 'titles_to_id_udf' over the links column of the 'data' DF for obtaining the IDs of the outgoing links.\n",
    "Partialdirect_linksDF= data.select(data[\"id\"],titles_to_id_udf(data[\"links\"]).alias(\"links_id\")).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b11e56f0-6677-4b4f-ab29-9bc2a4bcd81d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** Step 3: Create the Count DF ** <br>\n",
    "Create another UDF which counts the lengths of the lists of the previous computed column 'links_id'. Conceptually, these counts are the number of outgoing links which are in the text of the pages in the 'ids' column. \n",
    "We will generate a different Data Frame for storing these results. Once we have created the Spark DF, we will pass it to pandas and make it a broadcast Variable (good idea since it is a static dataframe). \n",
    "\n",
    "\n",
    "At a beginning, I didn´t perform this transformation. What I did before was to create a count column in the Reverse DataFrame (which contained the counts of the links which pointed to our page of interest). <br>\n",
    "The reason why I have decided to do it in this way instead is because it takes much less time to perform the final 'while' loop due to how Spark works. What I mean is that Spark doesn´t make the computations up to we use a df.toPandas() for instance ( or also --> df.collect(), dislplay(df) ). It is true that takes time for transforming to Pandas but in the long run it comes out being much more efficient. \n",
    "\n",
    "Count DF:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>Partialdirect_links_count_DF</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>ids:integer</td>\n",
    "  <td>count:integer</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a09c90e6-afd5-4a85-8f0c-4654355e47df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UDF for counting the number of links that each page contains. It just takes the length of the links_id list \n",
    "def count_links(links):\n",
    "  return len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "79634ab1-b1ea-45d6-a5f5-e091e61f4d1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_links_udf = udf(count_links,IntegerType())  # transform to UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "70a4e645-0665-4388-bb32-ccd205178b83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate a DF which has the id of the page and the number of outgoing links it has  (the 'count' var)\n",
    "Partialdirect_links_count_DF=Partialdirect_linksDF.select(Partialdirect_linksDF[\"id\"].alias('ids'),count_links_udf(Partialdirect_linksDF[\"links_id\"]).alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0bd3d670-97a0-4f09-a4d2-1f3d8358f02e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_pd = Partialdirect_links_count_DF.toPandas()  # To pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1588d6f-c148-4def-a9a8-ed07962bed0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_broad= sc.broadcast(count_pd)    # Making it Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8a69d1fc-f9a1-486a-85fc-f891f052971a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** Step 4: Creating the Reverse DF ** <br>\n",
    "\n",
    "Since for the PR compuation we need to know the pages which point to our actual page, we need to obtain the **Reverse DF** from the 'Partialdirect_linksDF'.<br>  A simple way of understanding how this DF is, imagine an instance of the direct DF :\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>Direct DF</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <th>id (points to links in 'link_id')</th>\n",
    "  <th>link_id</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>id_23</td>\n",
    "  <td>[id_10,id_34]</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "In the Reverse DF it would be tranformed into two instances :\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>Reverse DF</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <th>id (pointed by links in 'link_id')</th>\n",
    "  <th>link_id</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>id_10</td>\n",
    "  <td>id_23</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>id_34</td>\n",
    "  <td>id_23</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<br>For making this transformation I use the 'explode' function which basically divides the instances (From: id_23 --> [id_10,id_34] ; To: id_23 --> id_10 and id_23 --> id_34). After using 'explode', I make a shift in the columns and the reverse would be done. The only thing left would be to group by the new 'id' column since can happen that other links are poinitng to the links in 'id' (imagine id_50 --> [id_10] ; Apply Reverse: id_10 --> id_50; Apply Groupby --> id_10 --> [id_23,id_50]).\n",
    "\n",
    "\n",
    "\n",
    "Also, it would be very usefull to have the count attribute of those pages which point to other (in the above example, the count of id_23 would be 2).\n",
    "However, because of computational reasons (as explained in Step 3), I have decided just to leave the Reverse DF with the two necessary Columns:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>Partialreverse_linksDF</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>id:integer</td>\n",
    "  <td>link_id: array of integers (the IDs which point to id)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Something we must take into account is that, in the 'id' column, we don´t have all the pages of our wikipedia fraction. We just have those nodes which are pointed by others. That is why in the next step we must obtain an updated 'Partialreverse_linksDF' with all the pages of our subset (called 'Complete_df')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a51e1223-398f-4e7b-8c52-a7ee47f86aa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# TRANSFORMING THE DIRECT DF INTO REVERSE DF \n",
    "# I decided to apply the explode function because can be used for making the reverse in a faster way. \n",
    "Partialreverse_links= Partialdirect_linksDF.select(\"id\",explode(\"links_id\").alias('link_id'))\n",
    "Partialreverse_links = Partialreverse_links.select(Partialreverse_links[\"link_id\"].alias(\"id\"),Partialreverse_links[\"id\"].alias(\"links_id\"))\n",
    "\n",
    "# GROUP BY \n",
    "# Group by the column id (since there will be more than one id in the 'link_id' column that is pointing to the same page in the 'id' column)\n",
    "Partialreverse_linksDF = Partialreverse_links.groupBy(\"id\").agg(collect_list(\"links_id\").alias('link_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bcdb4668-058b-4f6e-ae2e-e63ba2129da7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** Step 5: Creating a DF with all the Nodes in the Subset ** <br>\n",
    "\n",
    "For obtaining a column with all the ids of the subset, we can take the id column of the 'Partialdirect_linksDF' (or from 'Partialdirect_links_count_DF' since it is the same) and of the 'Partialreverse_linksDF' and make its union (avoiding duplicate ids). This will give us a DF with a column containing all the ids (called 'result' in the code). <br>\n",
    "Then we have to join the result DF with 'Partialreverse_linksDF' (by their id column) obtaining a very similar DF as 'Partialreverse_linksDF' but with a larger id column. Those ids which weren´t before in the 'Partialreverse_linksDF' now have NaN values in the column 'link_id'.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>Complete_df</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>id:integer</td>\n",
    "  <td>link_id: array of integers (the IDs which point to id; will contain NaN values)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "same as 'Partialreverse_linksDF' but rememeber keep in mind that 'Complete_df' has more instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "114b5130-0610-402f-9ac1-bd4d2ce1a84a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OBTAINING THE IDS OF ALL THE PAGES IN OUR SUBSET\n",
    "rev_id = Partialreverse_linksDF.select(Partialreverse_linksDF[\"id\"])  # IDs from the Reverse DF \n",
    "dir_id = Partialdirect_links_count_DF.select(Partialdirect_links_count_DF[\"ids\"]) # IDs from the Direct DF \n",
    "\n",
    "# Union of the ids of Partialreverse_linksDF and  Partialdirect_links_count_DF (which has the same id column as Partialdirect_linksDF)\n",
    "result = rev_id.union(dir_id).distinct()  # With distinct we avoid duplicates\n",
    "result = result.select(result.id.alias('ids'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ae63e14-97b4-4834-b3d1-3e842302c1c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# OBTAIN THE FULL REVERSE DF \n",
    "# Join with the Reverse DF for having the 'link_id' column information. Two situations will happen:\n",
    "#  'id'        'link_id'      (columns)\n",
    "# id_20 <-- [id_69, id_39]  This node is pointed by others\n",
    "# id_2  <--       NaN       This node isn´t pointed by other pages\n",
    "\n",
    "Complete_df = result.join(Partialreverse_linksDF,result.ids == Partialreverse_linksDF.id,\"full\")  \n",
    "\n",
    "# Null values are generated but no problem, they will be usefull for differentiate which ids aren´t pointed by others  (null == None)\n",
    "Complete_df = Complete_df.select(Complete_df['ids'].alias('id'),Complete_df['link_id']).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4301932e-4a6e-452f-89a8-34baa81474a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** Step 6: Initialize the Pandas DF (containing the initial PR) ** <br>\n",
    "\n",
    "The Page Rank column that we are going to calculate at each iteration will be needed for the next iteration. That is why it would be of interest having this information in an accessible variable. That is why we are going to create a Pandas DF with all the pages´ id and their respective PR.\n",
    "Taking advantage of the fact that we are passing it to pandas, we can obtain very easily and quickly the total number of pages that our subset has (N). In case we wanted to obtain this value from a spark dataframe it would be much more expensive. <br>\n",
    "After obtaining N, we can initiallize easily the PR column (all with 1/N values). \n",
    "\n",
    "Since we will need to update our Pandas DF at the end of every iteration, make it a broadcast variable isn´t a good idea in this case.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>partial_pandas</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>id:integer</td>\n",
    "  <td>PR_prev:integer</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "041e07e8-6737-4f41-bf4c-5792e570edf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform to pandas the column of ids (contains all the ids in the subset)\n",
    "result_ = result.select(result['ids'].alias('id'))\n",
    "partial_pandas = result_.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "64816d95-0640-4677-a118-d72fb22de41f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len_col = len(partial_pandas.index)  # Computing the value of N \n",
    "partial_pandas['PR_prev'] = 1/len_col  # Initializing the initial PR column (all with 1/N values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e257326-2e27-438e-ac21-7e86b3b54df7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partial_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "313ed0a7-252c-44ea-b563-9865532262b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** Step 7: UDF for computing the Page Rank of each page ** <br>\n",
    "\n",
    "Now we are ready to compute the Page Ranks. We will code a UDF called 'iterat', which by imputing the column 'links_id' and the 'partial_pandas' DF (updated at each iteration), returns a column with the  updated Page Rank. \n",
    "\n",
    "***As has been explained before, we have two situations:*** <br>\n",
    " * *Pages which are pointed by others:* <br>In this case, first we will need to find the PRs of the links which are pointing to the page of interest (that is, the Page Ranks belonging to the links inside the 'links_id' list). We will search the PRs of the links in the Pandas DF. Afterwards, we will need to find the count values of the links which are pointing to the page of interest (use the Count Broadcast variable 'count_broad' for doing this). <br>Once we have the PRs (saved in a list called 'link_id_pr') and the count values, we perform the one by one division of the lists 'link_id_pr' and 'count' (i.e. link_id_pr= [0.003,0.001]; count = [4,7] --> division_list = [0.003/4,0.001/7]). <br>  Having the list of quotients, we add its values and apply the dumping factor criterion. That would give us the updated PR ('PR_actual' in code).\n",
    " \n",
    " * *Pages which aren´t pointed by others:* <br>For these pages, we can´t compute the summatory of quotients, since they aren´t pointed by other pages. As a result, the value of this sum is 0. Thus we can only update the PR of these pages by using \\\\(\\\\frac{d}{N}\\\\). This is why the page rank for these pages changes only once, then iteration after iteration they keep the same PR (and thus their difference with the previous PR is 0). \n",
    " \n",
    " \n",
    "We will have to refresh this UDF at every iteration, because if not, it won´t work with the updated pandas of each iteration (it will only use the initial one, with all PRs 1/N). \n",
    "\n",
    "Resulting DF (this DF will be converted to Pandas for updating the 'partial_pandas' DF):\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>secondary_DF</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>id:integer</td>\n",
    "  <td>PR_prev:float</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Initially, I also computed the difference with the previous PR (and returned a list with the updated PR and the difference). But for computational reasons I decided to take advantage of the pandas DFs for obtaining the differences (In the next Step is all explained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a22f6a80-9aec-4e76-8b10-e19f9a314ac1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dumping = 0.85   # Dumping Factor\n",
    "def iterat(links, prt_pd = partial_pandas):     # 'links' correspond to each instance in the 'link_id' column\n",
    "   \n",
    "    # In case it is a page that isn´t pointed by others\n",
    "    if links == None:\n",
    "      PR_actual = (dumping)/len_col  # New Page Rank \n",
    "      return float(PR_actual)\n",
    "    \n",
    "    # In case it is a page that is pointed by others\n",
    "    else:   \n",
    "      # Find the Page ranks of the ids pointing to the id of interest (that is, the Page Ranks of the ids inside the list 'links')\n",
    "      a = prt_pd[prt_pd.id.isin(links)]  \n",
    "      pr_p = a.PR_prev\n",
    "      link_id_pr = list(map(float, pr_p))  # List of Page Ranks \n",
    "      \n",
    "      # Find the count of outgoing links of the ids pointing to the id of interest (that is, the counts of the ids inside the list 'links')\n",
    "      count_list = count_broad.value[count_broad.value.ids.isin(links)]\n",
    "      cl = count_list['count'].to_list()\n",
    "      count = list(map(int, cl))     # List of Count values\n",
    "      \n",
    "      # Compute the individual quotients\n",
    "      act_PR = list(map(truediv, link_id_pr, count)) #--> link_id_pr/count (these are lists)  link_id_pr: list of Page ranks | count: list of counts\n",
    "    \n",
    "      PR_actual = (dumping/len_col)+ ((1-dumping)*sum(act_PR))   # New Page Rank \n",
    "      return float(PR_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "844e61c4-a357-4320-93ee-5db4d21a5bf8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** Step 8: Iterative Computation of the Page Rank ** <br>\n",
    "\n",
    "At this point, we already have everything we need for the PR computation. We must perform a while loop which stops either when all the values in the column of differences are smaller than the tolerance (0.001) or when the maximum number of iterations (20) is reached. <br>\n",
    "What I have done for checking if the loop must stop by tolerance is to initialize a boolean value as True (condit) which only will turn to False if the maximum number of the 'Diff' column is smaller than the tolerance. In this way, when the maximum is smaller than the tolerance, that implies that the rest of the differences are also smaller.\n",
    "\n",
    "***What is repeated at each iteration are the following steps: ***\n",
    "\n",
    " * Step 0: Update the 'iterat' UDF (explained in Step 7) with the corresponding 'partial_pandas' DF at each iteration. \n",
    "\n",
    " * Step 1: Apply the 'iteration_udf' UDF as has been explained before. The DF 'secondary_DF' is obtained (with 'id' and 'PR_prev', which is the updated page rank, but for code reasons is named like that) and then is transformed to a Pandas DF (next_pandas).\n",
    " \n",
    " * Step 2: Now, since we have generated a new pandas with the updated PR column, we can obtain the 'Diff' column in a very quick way, by substracting the partial_pandas PR_prev column and the next_pandas PR_prev column. Obtaining the maximum value by this procedure is much faster than computing it in the Spark DF and then finding the max of this column (because spark is going to perform all the computations when we use the 'collect' tool). After obtaining the 'Diff' column we just have to find the maximum of this and check if is higher or equal than the tolerance. \n",
    "\n",
    " * Step 3: Finally, we must update the 'partial_pandas' DF with the values of the 'next_pandas' DF, since for the next iteration, the new PRs are computed with the PRs of 'next_pandas' DF. Also we increase by one the counter 'iteration'. These would be the steps to perform at each iteration. \n",
    "  \n",
    "Once the while loop finishes, we will just have to display the result in a clear way:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "  <th>Final_display</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>title:string</td>\n",
    "  <td>id:integer</td>\n",
    "  <td>Page_Rank:float </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "66b23412-5282-4e4c-b1a2-3dfe82a39bdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the necesary parameters \n",
    "tolerance = 0.001 # For stopping by tolerance condition\n",
    "condit = True  # Boolean var for stopping by tolerance\n",
    "max_iterations = 20 # For stopping by iteration condition\n",
    "iteration = 0  # Counter for iterations\n",
    "\n",
    "while (condit and iteration < max_iterations):\n",
    "      # Step 0\n",
    "      iteration_udf = udf( lambda links: iterat(links,partial_pandas), DoubleType())  # Update the UDF for computing the PRs\n",
    "      \n",
    "      # Step 1\n",
    "      secondary_DF = Complete_df.select('id',iteration_udf('link_id').alias('PR_prev')) # Computing the new PRs\n",
    "      next_pandas = secondary_DF.toPandas()   # Transform to Pandas the previous DF \n",
    "      \n",
    "      # Step 2\n",
    "      # Obtain the Diff column (the result of this computation is a Pandas column with all the differences)\n",
    "      diff = abs(partial_pandas.PR_prev - next_pandas.PR_prev)/partial_pandas.PR_prev \n",
    "      condit = max(diff) >= tolerance # Obtain the maximum of this column and compare with tolerance \n",
    "      \n",
    "      # Step 3\n",
    "      partial_pandas = next_pandas   # Update the 'partial_pandas' DF substituting it by 'next_pandas'\n",
    "      iteration += 1  # Increase counter\n",
    "\n",
    "print('Number of Iterations: ',iteration)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e69cb53e-ffa0-4d49-8958-2b7e0b472ded",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Page Rank Sum Aclaration**\n",
    "\n",
    "If the subset on which we execute the code is very small, then when we sum all the final PRs, we will notice that the result of this sum  tends towards the dumping factor. This makes sense, since in a small subset there are a lot of pages which aren´t pointed by others (remember that the PR for these pages is (dumping factor/N), thus the final PR sum tends to 0.85 (in case we are using a dumping factor of 0.85).\n",
    "\n",
    "As long as we take a subset with more instances, the final sum of PRs will tend to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06c81653-46cb-432e-b72b-5ad54c077d9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "val = partial_pandas[\"PR_prev\"].to_list()    # Takig the PR_prev column of the Pandas DF as list, for then making the sum in a very easy way\n",
    "print('Total Sum of Page Ranks: ',sum(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6f7be9d1-624f-41bf-91b3-c1f0442c48c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is a simple UDF for searching in the broadcast var the respective titles of the ids in the 'id' col. \n",
    "def title_id (id_):\n",
    "  elem = broadcastVar.value[broadcastVar.value.id.isin([id_])]\n",
    "  return elem.title.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b1b0b7b5-ffee-4ca7-8188-ddcbb83e375c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "title_udf = udf(title_id, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1cab6efe-b763-46e3-b6ab-360480d08da6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DF for making the Display of the results \n",
    "Final_display = secondary_DF.select( title_udf(secondary_DF['id']).alias('title'),'id',(secondary_DF.PR_prev).alias('Page_Rank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1b53f5f6-d3b3-4c0c-8b63-ebf253ac1334",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(Final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b15042ff-2050-445f-a9ed-1d0cd7bc9d60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "After having done this work, it has become more than clear to me that spark is a great tool for solving Big Data problems. In my opinion, the biggest advantage it has is the fact that code can be executed by more than one device (if it is coded with Spark structures in an appropriate way). <br> For a basic laptop like mine, it takes much time in running all the code for a relatively small fraction of data. I guess that for a more powerfull computer (which can handle a bigger amount of workers) it can run this code for a bigger fraction of data in less time even. \n",
    "\n",
    "However, there are other problems which require the use of much more instances than the total of instances of this problem (5823210) and in those cases it would be unfeasible to execute the code with only one computer, no matter how powerful it is. \n",
    "\n",
    "I´m sure that in the future we will have to work with Spark or similar tools, since today, to solve a problem in the most effective way a huge amount of data needs to be processed."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "FinalP_def",
   "notebookOrigID": 3885450204724535,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
